{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cbbe360",
   "metadata": {},
   "source": [
    "## Импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b0bf72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from wordfreq import top_n_list, zipf_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c06a7e",
   "metadata": {},
   "source": [
    "## Создание частотного словаря"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36040300",
   "metadata": {},
   "source": [
    "Для решения задачи, нам необходимо создать словарь, где для каждого слова будет указана его частота встречаемости в языке\n",
    "Алгоритм будет опираться на эти частоты, чтобы выбрать наиболее вероятное разбиение строки на слова\n",
    "\n",
    "Берем по 800,000 самых частых слов из русского и английского языков и создаем на этой основе словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea1b7c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузим частоты для русского и английского языков\n",
      "Создан словарь. Всего слов: 995563\n",
      "Файл 'ru_en_dict.tsv' создан\n"
     ]
    }
   ],
   "source": [
    "print(\"Загрузим частоты для русского и английского языков\")\n",
    "words_ru = top_n_list('ru', 800_000)\n",
    "words_en = top_n_list('en', 800_000)\n",
    "\n",
    "# Создаждим словарь частот\n",
    "freq = {w: zipf_frequency(w, 'ru') for w in words_ru}\n",
    "for w in words_en:\n",
    "    if w not in freq:\n",
    "        freq[w] = zipf_frequency(w, 'en')\n",
    "\n",
    "print(f\"Создан словарь. Всего слов: {len(freq)}\")\n",
    "\n",
    "# Сохраняем словарь\n",
    "with open('ru_en_dict.tsv', 'w', encoding='utf-8') as f:\n",
    "    for w, z in sorted(freq.items(), key=lambda x: -x[1]):\n",
    "        f.write(f\"{w}\\t{z:.3f}\\n\")\n",
    "\n",
    "print(\"Файл 'ru_en_dict.tsv' создан\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b9ad60",
   "metadata": {},
   "source": [
    "## Вспомогательные функции"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac9d23",
   "metadata": {},
   "source": [
    "Функция загрузки созданного нами словаря "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c3a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_freq_dict(dict_path='ru_en_dict.tsv'):\n",
    "    print(f\"Загрузка словаря частот из файла '{dict_path}'\")\n",
    "\n",
    "    if not os.path.exists(dict_path):\n",
    "        print(f\"Ошибка. Файл словаря '{dict_path}' не найден\"); return None\n",
    "    \n",
    "    freq_dict = {}\n",
    "    one_letter_whitelist = {'в', 'с', 'и', 'к', 'у', 'а', 'a', 'i'}\n",
    "\n",
    "    with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                word, freq = parts\n",
    "                if len(word) > 1 or word in one_letter_whitelist:\n",
    "                    freq_dict[word] = float(freq)\n",
    "    \n",
    "    for word in one_letter_whitelist:\n",
    "        if word not in freq_dict: freq_dict[word] = 5.0\n",
    "        \n",
    "    print(f\"Словарь готов. Уникальных слов: {len(freq_dict)}\")\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4bffa1",
   "metadata": {},
   "source": [
    "### Опишем функции алгоритма сегментации\n",
    "\n",
    "Функция segment_text:\n",
    "Для каждого префикса строки она находит самое вероятное разбиение на слова, основываясь на частотах из словаря.\n",
    "Качество разбиения оценивается как сумма частот слов, умноженных на бонус за длину слова. Бонус помогает алгоритму предпочитать более длинные и осмысленные слова вместо дробления на короткие куски.\n",
    "Если слово не найдено в словаре, ему присваивается большой отрицательный штраф, чтобы алгоритм избегал таких вариантов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efd315f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_text(text, freq_dict):\n",
    "    n = len(text)\n",
    "    memo = {0: (0.0, 0)}\n",
    "    unknown_word_freq = -20.0\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        best_score = -float('inf'); best_len = 0\n",
    "        max_word_len = min(i, 25) \n",
    "\n",
    "        for k in range(1, max_word_len + 1):\n",
    "            start_pos = i - k; word = text[start_pos:i]\n",
    "            word_len_bonus = len(word) ** 2\n",
    "            base_freq = freq_dict.get(word, unknown_word_freq)\n",
    "            current_word_score = base_freq * word_len_bonus\n",
    "            prev_score, _ = memo.get(start_pos, (-float('inf'), 0))\n",
    "            current_total_score = prev_score + current_word_score\n",
    "\n",
    "            if current_total_score > best_score:\n",
    "                best_score = current_total_score; best_len = k\n",
    "\n",
    "        memo[i] = (best_score, best_len)\n",
    "\n",
    "    words = []\n",
    "    i = n\n",
    "\n",
    "    while i > 0:\n",
    "        _, k = memo[i]\n",
    "        if k == 0: break\n",
    "        words.append(text[i-k:i]); i -= k\n",
    "\n",
    "    return words[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae3187c",
   "metadata": {},
   "source": [
    "\n",
    "Функция segmenter:\n",
    "Будет обрабатывать строки, которые содержат содержат русские буквы, латиницу, цифры и знаки препинания.\n",
    "1. Текст будет разбиваться на островки кириллицы, латиницы, цифр и прочих символов\n",
    "2. Каждый \"островок\" кириллицы или латиницы будет обрабатываться через функцию segment_text\n",
    "3. Цифры и символы добавляем как есть\n",
    "4. В конце сделаем склейку, чтобы знаки препинания присоединялись к предыдущему слову"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fcb3a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmenter(text, freq_dict):\n",
    "    # Найдем блоки кириллицы, латиницы, цифр и тд\n",
    "    pattern = r'([а-яё]+|[a-z]+|[0-9]+|[^а-яёa-z0-9\\s]+)'\n",
    "    parts = re.findall(pattern, text.lower())\n",
    "    \n",
    "    raw_tokens = []\n",
    "    for part in parts:\n",
    "        if not part: continue\n",
    "        # Если часть - это кириллица ИЛИ латиница, сегментируем ее\n",
    "        if re.fullmatch(r'[а-яё]+', part) or re.fullmatch(r'[a-z]+', part):\n",
    "            raw_tokens.extend(segment_text(part, freq_dict))\n",
    "        # Иначе (цифры, символы) - добавим как есть\n",
    "        else:\n",
    "            raw_tokens.append(part)\n",
    "\n",
    "    if not raw_tokens:\n",
    "        return []\n",
    "\n",
    "    # Приклеим пунктуацию\n",
    "    final_tokens = [raw_tokens[0]]\n",
    "    punctuation_pattern = r'^[.,!?;:)\\]}]+$'\n",
    "    for token in raw_tokens[1:]:\n",
    "        if re.fullmatch(punctuation_pattern, token):\n",
    "            if final_tokens:\n",
    "                final_tokens[-1] += token\n",
    "        else:\n",
    "            final_tokens.append(token)\n",
    "            \n",
    "    return final_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff7020",
   "metadata": {},
   "source": [
    "## Обработка и создание submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b83bc5c",
   "metadata": {},
   "source": [
    "Вызовем все описанные выше функции по очереди\n",
    "\n",
    "1. Загрузим частотный словрь\n",
    "2. Для каждой строки из task_data.txt вызывает segmenter и получает список слов.\n",
    "3. На основе списка слов вычисляет позиции, где должны стоять пробелы.\n",
    "4. Собирает все результаты в список submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f3267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    word_freq_dict = load_word_freq_dict('ru_en_dict.tsv')\n",
    "    if word_freq_dict is None: return\n",
    "\n",
    "    input_file = 'task_data.txt'\n",
    "\n",
    "    ids, texts = [], []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        next(f);\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            parts = line.split(',', 1)\n",
    "            ids.append(parts[0]); texts.append(parts[1] if len(parts) > 1 else \"\")\n",
    "    data = pd.DataFrame({'id': ids, 'text_no_spaces': texts})\n",
    "\n",
    "    all_results_for_submission = []\n",
    "\n",
    "    for _, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        text_id = int(row['id'])\n",
    "        text_no_spaces = str(row['text_no_spaces'])\n",
    "        \n",
    "        segmented_words = segmenter(text_no_spaces, word_freq_dict)\n",
    "        \n",
    "        predicted_positions = []\n",
    "        current_pos = 0\n",
    "        if len(segmented_words) > 1:\n",
    "            for i in range(len(segmented_words) - 1):\n",
    "                current_pos += len(segmented_words[i])\n",
    "                predicted_positions.append(current_pos)\n",
    "        \n",
    "        all_results_for_submission.append({'id': text_id, 'predicted_positions': str(predicted_positions)})\n",
    "        \n",
    "    submission_df = pd.DataFrame(all_results_for_submission)\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    print(\"Файл 'submission.csv' создан.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed180494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка словаря частот из файла 'ru_en_dict.tsv'...\n",
      "Словарь готов. Уникальных слов: 994310\n",
      "Чтение через pandas не удалось, пробуем вручную...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1005/1005 [00:00<00:00, 6429.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл 'submission.csv' создан.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-split-word",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
